
# CNN for speech denoising

This project is the part of my homework in [My first data project](https://ods.ai/tracks/my_first_data_project) course
This project is part of my homework on [My first data project](https://ods.ai/tracks/my_first_data_project) course.

Основная цель представленного проекта - решить задачу очистки речи от фоновых шумов, используя технологии машинного обучения.

В описании ниже представлено обоснование выбранной архитектуры, приведены результаты экспериментов по подбору оптимальных гиперпараметров, описан процесс обучения нейронной сети.

## Основные библиотеки

- librosa
- numpy
- tensorflow

## Содержание

Репозиторий состоит из нескольких Jupyter ноутбуков:
- cnn_model_final - описание модели и процесса ее обучения. 
- cnn_model_test - демонстрация работы модели на любом аудио.
- prepare_dataset - ноутбук для преобразования данных для модели. Версия тут - обработка единственной аудиозаписи.

И нескольких .py файлов: по сути простой перенос cnn_model_final в нормальный вид.

speech_model.h5 - самая последняя версия обученной модели.

## Dataset

Машинное обучение начинается с данных. Итоговый датасет для задачи был сгенерирован на основе следующих наборов:
- Common Voice (https://commonvoice.mozilla.org/ru/datasets). Имеет большое число проверенных аудиозаписей - зачитанных спикерами текстов.
- UrbanSound8K (https://www.kaggle.com/datasets/chrisfilo/urbansound8k). Большой объем разнотипных городских шумов.

Из каждой аудиозаписи отсекается тихий участок (в начале и конце аудиозаписи). Далее, на записи голоса (1 датасет) накладываются выбранные случайным образом (из 2 датасета) шумы, с параметром SNR в диапазоне от 0 до 15 db. После, с преобразованием Фурье, каждое аудио конвертируется в STFT magnitude vectors, которые, после некоторых дополнительных шагов, подаются в сеть.

*SNR (Signal-to-Distortion) - отношение мощности полезного сигнала к мощности шума.

Детали описаны в статье [1], ссылка в конце документа.


## Testing
[Example 1](https://drive.google.com/drive/folders/1-5wGQ1fpA0poQMsjk5xdNzo99HKL01lp?usp=sharing):

- STOI: 0.7485873492849783
- SDR: 11.094792287825626

![Screenshot from 2023-03-21 12-10-12](https://user-images.githubusercontent.com/63301430/226561548-743503a6-c1c2-42dc-a7f5-a0f7a2a508e5.png)
![Screenshot from 2023-03-21 12-10-23](https://user-images.githubusercontent.com/63301430/226561551-b33b70f7-f699-4879-a481-b6cf86470f31.png)
![Screenshot from 2023-03-21 12-10-28](https://user-images.githubusercontent.com/63301430/226561554-a6e60551-fac5-4cce-a79f-03731b4b90a5.png)

[Example 2](https://drive.google.com/drive/folders/1-5wGQ1fpA0poQMsjk5xdNzo99HKL01lp?usp=sharing):

- STOI: 0.5552426231950904
- SDR: 6.131914852996063

![Screenshot from 2023-03-21 12-15-12](https://user-images.githubusercontent.com/63301430/226562559-49c322a3-66df-4c48-886a-ddcc3c02cc1a.png)
![Screenshot from 2023-03-21 12-15-18](https://user-images.githubusercontent.com/63301430/226562563-de83326d-36c4-46af-aac9-bea6fdec9fee.png)
![Screenshot from 2023-03-21 12-15-23](https://user-images.githubusercontent.com/63301430/226562569-27601b24-8d6d-4be8-bc8c-f0099023370e.png)
## Подбор гиперпараметров

Сперва стоит отметить, что автор работы столкнулся с техническими ограничениями и не смог организовать обучение моделей с большей вариативностью в значениях гиперпараметров. Эксперименты будут продолжены.

## Инструменты
В качестве основного инструмента версионирования экспериментов была взята платформа Weights and Biases. Процесс подбора гиперпараметров проводился с использованием ее особенностей [sweeps](https://docs.wandb.ai/guides/sweeps) и метода баесовской оптимизации.
Все эксперименты доступны к просмотру в проекте speech_denoising (можно найти в профиле - [ссылка](https://wandb.ai/sams3pi01?shareProfileType=copy)).

Поговорим о самом процессе: 

#### Архитектура нейронной сети
Пара слов об архитектуре, поскольку она имеет прямое влияние на гиперпараметры. 

Архитектура заимствована из уже упомянутой выше статьи [1] (следующие снимки оттуда) по нескольким причинам:
- Исследование, описанное в работе, проводилось с целью найти оптимальное с точки зрения числа параметров и качества решение, которое бы функционировало в условиях серьезных аппаратных ограничений (в статье - на слуховом аппарате).
- В исследовании проведено сравнение нескольких архитектур нейронных сетей, выбор был сделан по его итогам.
- CNN демонстрирует высокую производительность в том числе на задачах связанных с обработкой речи, что полезно при размещении модели на смартфоне.
- Уровень сложности реализации.
- Автор данной статьи проявляет интерес к computer vision и обработке изображений. 

*Сравнение работы разных архитектур нейронных сетей в статье:
снимочек - 1
снимочек - 2
Выбрана CR-CED (skip) поскольку, имея наименьшее число параметров, модель демонстрирует неплохие результаты.

Таким образом, число нейронов и число слоев, metric - rmse (широко распространенная для задач регресии, к слову), activation - adam, loss - mse взяты из статьи.

*Если такой подход не приемлем в рамках курса, постараюсь успеть разработать полностью свое решение.*

## Batch-size

Пока выдвину предположение, что оптимальное значение данного параметра или 256, или 502. 
Предположение основано на изучении сторонних источников и самой успешной попытке обучить модель (ибо там значение лежит в этом диапазоне).

## Learning rate

Изначально значение данного параметра было приравнено к 0.0015, поскольку именно это число приводилось в статье. Однако, из-за разных размеров датасетов (4000 против 1000 - 1500 на эксперимент в нашем случае), стало очевидно, что оно должно быть увеличено, в противном случае, процесс обучения требовал бы еще больше времени и данных.

По итогу, модель со значением параметра в 0.0029 (выбранного, в рамках баесовой оптимизации, случайным образом), показала неплохие результаты и по времени, и по скорости обучения, и по точности модели в результате.

*Следует упомянуть, что в статье приводился вариант обучения лесенкой(?): обучение проводится до тех пор, пока значения validation loss не упираются в какое-либо плато -> полученные веса модели используются на следующем этапе обучения, где значение гиперпараметра принимается за 0.00075 (т.е. делится на два). И следующие шаги выполняются похожим образом (learning_rate / 3, learning_rate / 4). 
Этот способ стал откровением для автора и он не мог его не проверить, см рисунок ниже. 

*Действительно, следование этому алгоритму позволяет достичь неплохих результатов, хоть и требует дополнительных временных вложений.

## Epochs

Число эпох сильно зависит от приведенных выше гиперпараметров. Пока в них нет предельной ясности, не удастся дать точный ответ и на этот пункт.

Пока за оптимальное значение считается промежуток от 16 до 64. На практике, модели с числом эпох от 30 до 40 показывали неплохие результаты, но делать выводы, как уже сказано неоднократно, рано.

## References
- https://paperswithcode.com/paper/a-fully-convolutional-neural-network-for
- https://habr.com/ru/post/668518/
- https://www.youtube.com/watch?v=ZqpSb5p1xQo&list=WL&index=8&t=1072s
- https://www.mathworks.com/help/deeplearning/ug/denoise-speech-using-deep-learning-networks.html
- https://www.kaggle.com/code/danielgraham1997/speech-denoising-analysis#Metrics-Analysis
- https://www.kaggle.com/code/carlolepelaars/bidirectional-lstm-for-audio-labeling-with-keras/notebook
- https://www.kaggle.com/datasets/chrisfilo/urbansound8k
- https://commonvoice.mozilla.org/

##  Примеры

Примеры могут быть найдены в соответствующей папке репозитория.

## References
- https://paperswithcode.com/paper/a-fully-convolutional-neural-network-for
- https://habr.com/ru/companies/antiplagiat/articles/528384/
- https://habr.com/ru/post/668518/
- https://www.youtube.com/watch?v=ZqpSb5p1xQo&list=WL&index=8&t=1072s
- https://www.mathworks.com/help/deeplearning/ug/denoise-speech-using-deep-learning-networks.html
- https://www.kaggle.com/code/danielgraham1997/speech-denoising-analysis#Metrics-Analysis
- https://www.kaggle.com/code/carlolepelaars/bidirectional-lstm-for-audio-labeling-with-keras/notebook